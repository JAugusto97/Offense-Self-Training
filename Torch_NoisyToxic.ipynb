{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch_NoisyToxic.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Noisy Student NLP Experiment\n"
      ],
      "metadata": {
        "id": "ihYgOFMzMHZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "UZonXp87f_Ns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgCCQCUAuz-Z"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Dependencies"
      ],
      "metadata": {
        "id": "2ZN-nc1SgHqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gputil transformers emoji --quiet"
      ],
      "metadata": {
        "id": "1fdxqOyBcvXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tGwYu4ZCWxfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import RandomSampler, SequentialSampler, Dataset, DataLoader\n",
        "\n",
        "from scipy.special import softmax\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, get_scheduler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = False"
      ],
      "metadata": {
        "id": "ba3IPaucco-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Utility Functions"
      ],
      "metadata": {
        "id": "sF_wLqBogXz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Handling"
      ],
      "metadata": {
        "id": "CJ8q67c4jAEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mhs():\n",
        "    path = os.path.join(CFG.data_path, \"english\", \"measuring_hate_speech\")\n",
        "\n",
        "    unlabeled_path = os.path.join(CFG.data_path, \"english\", \"unlabeled\", \"tweets_augmented.csv\")\n",
        "    train_path = os.path.join(path, \"measuring_hate_speech.csv\")\n",
        "\n",
        "    data_df = pd.read_csv(train_path)\n",
        "    data_df.loc[data_df[\"hate_speech_score\"] >= 1, \"label\"] = 1\n",
        "    data_df.loc[data_df[\"hate_speech_score\"] < 1, \"label\"] = 0\n",
        "    data_df = data_df[[\"text\", \"label\"]]\n",
        "    data_df[\"label\"] = data_df[\"label\"].astype(int)\n",
        "\n",
        "    train_df, dev_df = train_test_split(data_df, train_size = 0.7, stratify=data_df[\"label\"], random_state=CFG.seed)\n",
        "    dev_df, test_df = train_test_split(dev_df, train_size = 0.5, stratify=dev_df[\"label\"], random_state=CFG.seed)\n",
        "\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    dev_df = dev_df.reset_index(drop=True)\n",
        "    test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "    unlabeled_df = pd.read_csv(unlabeled_path)\n",
        "     \n",
        "    return train_df, dev_df, test_df, unlabeled_df"
      ],
      "metadata": {
        "id": "plz7frl20f58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_convabuse():\n",
        "    path = os.path.join(CFG.data_path, \"english\", \"ConvAbuse\")\n",
        "\n",
        "    unlabeled_path = os.path.join(CFG.data_path, \"english\", \"unlabeled\", \"tweets_augmented.csv\")\n",
        "    train_path = os.path.join(path, \"ConvAbuseEMNLPtrain.csv\")\n",
        "    dev_path = os.path.join(path, \"ConvAbuseEMNLPvalid.csv\")\n",
        "    test_path = os.path.join(path, \"ConvAbuseEMNLPtest.csv\")\n",
        "\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    dev_df = pd.read_csv(dev_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    unlabeled_df = pd.read_csv(unlabeled_path)\n",
        "\n",
        "    train_df[\"text\"] = train_df.apply(lambda x: x[\"prev_agent\"] + \"\\n\" + x[\"prev_user\"] + \"\\n\" + x[\"agent\"] + \"\\n\" + x[\"user\"], axis=1)\n",
        "    dev_df[\"text\"] = dev_df.apply(lambda x: x[\"prev_agent\"] + \"\\n\" + x[\"prev_user\"] + \"\\n\" + x[\"agent\"] + \"\\n\" + x[\"user\"], axis=1)\n",
        "    test_df[\"text\"] = test_df.apply(lambda x: x[\"prev_agent\"] + \"\\n\" + x[\"prev_user\"] + \"\\n\" + x[\"agent\"] + \"\\n\" + x[\"user\"], axis=1)\n",
        "\n",
        "    train_df = train_df[[\"text\", \"is_abuse_majority\"]]\n",
        "    dev_df = dev_df[[\"text\", \"is_abuse_majority\"]]\n",
        "    test_df = test_df[[\"text\", \"is_abuse_majority\"]]\n",
        "     \n",
        "    return train_df, dev_df, test_df, unlabeled_df"
      ],
      "metadata": {
        "id": "afmxyH_hSl4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_olid():\n",
        "    eng_path = os.path.join(CFG.data_path, \"english\")\n",
        "\n",
        "    train_path = os.path.join(eng_path, \"OLIDv1.0\", \"olid-training-v1.0.tsv\")\n",
        "    test_path = os.path.join(eng_path, \"OLIDv1.0\", \"testset-levela.tsv\")\n",
        "    test_labels_path = os.path.join(eng_path, \"OLIDv1.0\", \"labels-levela.csv\")\n",
        "    unlabeled_path = os.path.join(eng_path, \"unlabeled\", \"tweets_augmented.csv\")\n",
        "\n",
        "    train_df = pd.read_csv(train_path, engine=\"python\", sep='\\t')[[\"tweet\", \"subtask_a\"]]\n",
        "    train_df[\"subtask_a\"] = train_df[\"subtask_a\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
        "    train_df = train_df.rename({\"tweet\": \"text\", \"subtask_a\": \"toxic\"}, axis=1)\n",
        "\n",
        "    test_df = pd.read_csv(test_path, engine=\"python\", sep='\\t')\n",
        "    test_labels = pd.read_csv(test_labels_path, header=None)\n",
        "    test_df[\"toxic\"] = test_labels[1].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
        "    test_df = test_df[[\"tweet\", \"toxic\"]]\n",
        "    test_df = test_df.rename({\"tweet\": \"text\"}, axis=1)\n",
        "\n",
        "    unlabeled_df = pd.read_csv(unlabeled_path)[[\"text\", \"text_augmented\"]]\n",
        "    unlabeled_df[\"text\"] = unlabeled_df[\"text\"]\n",
        "    unlabeled_df[\"text_augmented\"] = unlabeled_df[\"text_augmented\"]\n",
        "\n",
        "    unlabeled_df = unlabeled_df.drop_duplicates(\"text\")\n",
        "    \n",
        "    return train_df, None, test_df, unlabeled_df"
      ],
      "metadata": {
        "id": "PXJ4XBRqjExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_davidson():\n",
        "    path = os.path.join(CFG.data_path, \"english\", \"davidson\", \"davidson.csv\")\n",
        "    unlabeled_path = os.path.join(CFG.data_path, \"english\", \"unlabeled\", \"tweets_augmented.csv\")\n",
        "\n",
        "    data_df = pd.read_csv(path)\n",
        "    data_df.loc[data_df[\"class\"] != 0, \"label\"] = 1\n",
        "    data_df.loc[data_df[\"class\"] == 0, \"label\"] = 0\n",
        "    data_df[\"label\"] = data_df[\"label\"].astype(int)\n",
        "    data_df = data_df[[\"tweet\", \"label\"]]\n",
        "\n",
        "    train_df, dev_df = train_test_split(data_df, train_size = 0.7, stratify=data_df[\"label\"], random_state=CFG.seed)\n",
        "    dev_df, test_df = train_test_split(dev_df, train_size = 0.5, stratify=dev_df[\"label\"], random_state=CFG.seed)\n",
        "    unlabeled_df = pd.read_csv(unlabeled_path)\n",
        "\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    aux_train_df = train_df[train_df[\"label\"] == 0]\n",
        "    train_df = aux_train_df.append(train_df[train_df[\"label\"] == 1].sample(len(aux_train_df)))\n",
        "    dev_df = dev_df.reset_index(drop=True)\n",
        "    test_df = test_df.reset_index(drop=True)\n",
        "     \n",
        "    return train_df, dev_df, test_df, unlabeled_df\n"
      ],
      "metadata": {
        "id": "Kp62TinE_roU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stratified_split(df, num_split):\n",
        "    \"\"\"splits the dataset into 4 equal sized stratified parts and returns one of them\"\"\"\n",
        "    splits = []\n",
        "    left_half, right_half = train_test_split(df, train_size=0.5, shuffle=True, stratify=df.iloc[:, 1], random_state=CFG.seed)\n",
        "    splits.extend(train_test_split(left_half, train_size=0.5, shuffle=True, stratify=left_half.iloc[:, 1], random_state=CFG.seed))\n",
        "    splits.extend(train_test_split(right_half, train_size=0.5, shuffle=True, stratify=right_half.iloc[:, 1], random_state=CFG.seed))\n",
        "\n",
        "    return splits[num_split]\n"
      ],
      "metadata": {
        "id": "CQMGNbQxCuZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(few_shot=False, num_split=None):\n",
        "    if CFG.dataset_name == \"olidv1\":\n",
        "        train_df, dev_df, test_df, unlabeled_df = load_olid()\n",
        "    elif CFG.dataset_name == \"convabuse\":\n",
        "        train_df, dev_df, test_df, unlabeled_df = load_convabuse()\n",
        "    elif CFG.dataset_name == \"davidson\":\n",
        "        train_df, dev_df, test_df, unlabeled_df = load_davidson()\n",
        "    elif CFG.dataset_name == \"measuring_hate_speech\":\n",
        "        train_df, dev_df, test_df, unlabeled_df = load_mhs()\n",
        "\n",
        "    if few_shot:\n",
        "        train_df = get_stratified_split(train_df, num_split)\n",
        "\n",
        "    loaded_log = \\\n",
        "        f\"\"\"\\tLoaded {CFG.dataset_name}\"\"\"\n",
        "\n",
        "    if few_shot:\n",
        "        loaded_log += f\" - Split {num_split}\"\n",
        "\n",
        "    loaded_log += \\\n",
        "        f\"\"\"\\n\n",
        "        Train Size: {len(train_df)}\n",
        "            Positives: {len(train_df[train_df.iloc[:, 1] == 1])}\n",
        "            Negatives: {len(train_df[train_df.iloc[:, 1] == 0])}\n",
        "        \"\"\"\n",
        "\n",
        "    if dev_df is not None:\n",
        "        loaded_log += \\\n",
        "        f\"\"\"\n",
        "        Dev Size: {len(dev_df)}\n",
        "            Positives: {len(dev_df[dev_df.iloc[:, 1] == 1])}\n",
        "            Negatives: {len(dev_df[dev_df.iloc[:, 1] == 0])}\n",
        "        \"\"\"\n",
        "    loaded_log += \\\n",
        "        f\"\"\"\n",
        "        Test Size: {len(test_df)}\n",
        "            Positives: {len(test_df[test_df.iloc[:, 1] == 1])}\n",
        "            Negatives: {len(test_df[test_df.iloc[:, 1] == 0])}\n",
        "        Augmented Data: {len(unlabeled_df)}\n",
        "        \"\"\"\n",
        "    log(loaded_log)\n",
        "\n",
        "    return train_df, dev_df, test_df, unlabeled_df"
      ],
      "metadata": {
        "id": "S-g47q0hL9Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "T_1nRKEmjvFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, df, labels=None):\n",
        "        self.text = df[\"text\"].to_list()\n",
        "        self.text_augmented = df[\"text_augmented\"].to_list()\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels:\n",
        "            item = {\n",
        "                \"text\": self.text[idx],\n",
        "                \"labels\": self.labels[idx],\n",
        "                \"text_augmented\": self.text_augmented[idx]\n",
        "            }\n",
        "        else:\n",
        "            item = {\n",
        "                \"text\": self.text[idx],\n",
        "                \"text_augmented\": self.text_augmented[idx]\n",
        "            }\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)"
      ],
      "metadata": {
        "id": "WkLC9GUmjzjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "d5RIYpGTjZFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Bert Helpers"
      ],
      "metadata": {
        "id": "LNNOlIlPHacp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(train_dataloader, attention_dropout=None, classifier_dropout=None):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(CFG.pretrained_bert_name)\n",
        "\n",
        "    if \"distilbert\" in CFG.pretrained_bert_name:\n",
        "        if attention_dropout:\n",
        "            model.config.attention_dropout=attention_dropout\n",
        "        if classifier_dropout:\n",
        "            model.config.seq_classif_dropout=classifier_dropout\n",
        "\n",
        "    else:\n",
        "        if attention_dropout:\n",
        "            model.config.attention_probs_dropout_prob=attention_dropout\n",
        "        if classifier_dropout:\n",
        "            model.config.classifier_dropout=classifier_dropout\n",
        "\n",
        "    if CFG.weight_decay:\n",
        "      no_decay = ['bias', 'LayerNorm.weight']\n",
        "      optimizer_grouped_parameters = [\n",
        "          {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "          'weight_decay': CFG.weight_decay},\n",
        "          {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "      ]\n",
        "\n",
        "    model.to(CFG.device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate)\n",
        "\n",
        "    total_steps = len(train_dataloader) * CFG.num_train_epochs\n",
        "    num_warmup_steps = int(total_steps*CFG.warmup_ratio)\n",
        "\n",
        "    scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "DeDFvfHFkIi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    val_dataloader=None,\n",
        "    evaluate_during_training=False,\n",
        "    is_student=False,\n",
        "    unlabeled_dataloader=None,\n",
        "    unl_to_label_batch_ratio=None,\n",
        "):\n",
        "    progress_bar = tqdm(range(CFG.num_train_epochs * len(train_dataloader)))\n",
        "    print_each_n_steps = int(len(train_dataloader) // 4)\n",
        "    log(\"Start training...\\n\")\n",
        "\n",
        "    historic_loss = {\"loss\": [], \"labeled_loss\": [], \"unlabeled_loss\": [], \"steps\": [], \"unl_steps\": []}\n",
        "    for epoch_i in range(CFG.num_train_epochs):\n",
        "        if is_student:\n",
        "            log(\n",
        "                f\"{'Epoch':^7} | {'Labeled Batch':^14} | {'Unlabeled Batch':^16} | \"\n",
        "                f\"{'Train Loss':^11} | {'Labeled Loss':^13} | \"\n",
        "                f\"{'Unlabeled Loss':^15} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
        "            )\n",
        "            log(\"-\"*130)\n",
        "        else:\n",
        "            log(\n",
        "                f\"{'Epoch':^7} | {'Train Batch':^12} | \"\n",
        "                f\"{'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
        "            )\n",
        "            log(\"-\"*80)\n",
        "\n",
        "        # measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_unl_loss, batch_lab_loss, batch_counts, = 0, 0, 0, 0, 0\n",
        "\n",
        "        loss_list = []\n",
        "        unl_loss_list = []\n",
        "        lab_loss_list = []\n",
        "        step_list = []\n",
        "        unl_step_list = []\n",
        "\n",
        "        # train loop\n",
        "        model.train()\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            batch_inputs = {k: v.to(CFG.device) for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(**batch_inputs)\n",
        "            # if model is student, train with the noised data aswell\n",
        "            if is_student:\n",
        "                text_col = \"text_augmented\" if CFG.augmented_data else \"text\"\n",
        "                unl_logits = []\n",
        "                unl_labels = []\n",
        "\n",
        "                unl_losses_list = []\n",
        "                for i in range(unl_to_label_batch_ratio):\n",
        "                    unl_batch = next(iter(unlabeled_dataloader))\n",
        "                    unl_inputs = tokenizer.batch_encode_plus(\n",
        "                        unl_batch[text_col],\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        max_length=CFG.max_seq_len,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    unl_inputs[\"labels\"] = unl_batch[\"labels\"].clone().detach()\n",
        "                    unl_batch_inputs = {k: v.to(CFG.device) for k, v in unl_inputs.items()}\n",
        "                    unl_output = model(**unl_batch_inputs)\n",
        "\n",
        "                    unl_logits.append(unl_output.logits.cpu().detach().numpy())\n",
        "                    unl_labels.append(unl_inputs[\"labels\"].cpu().detach().numpy())\n",
        "\n",
        "                    del unl_batch_inputs\n",
        "                    del unl_output\n",
        "\n",
        "                # concatenate the unlabeled batch outputs into a single tensor\n",
        "                unl_labels = torch.cat([torch.as_tensor(t) for t in unl_labels])\n",
        "                unl_logits = torch.cat([torch.as_tensor(t) for t in unl_logits])\n",
        "\n",
        "                # combine unlabeled + labeled loss\n",
        "                unl_loss = loss_fn(unl_logits, unl_labels)\n",
        "                lab_loss = output.loss\n",
        "                loss = lab_loss + unl_loss\n",
        "\n",
        "                batch_lab_loss += lab_loss.item()\n",
        "                batch_unl_loss += unl_loss.item()\n",
        "\n",
        "            else:\n",
        "                loss = output.loss\n",
        "\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # historic data\n",
        "            loss_list.append(batch_loss/batch_counts)\n",
        "            step_list.append(step)\n",
        "            if is_student:\n",
        "                unl_loss_list.append(batch_unl_loss/batch_counts)\n",
        "                lab_loss_list.append(batch_lab_loss/batch_counts)\n",
        "                unl_step_list.append(unl_to_label_batch_ratio*step)\n",
        "\n",
        "            if CFG.clip_grad:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            progress_bar.update(1)\n",
        "\n",
        "            if (step % print_each_n_steps == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                if is_student:\n",
        "                    log(\n",
        "                        f\"{epoch_i + 1:^7} | {step:^14} | {(step*unl_to_label_batch_ratio):^16} | \"\n",
        "                        f\"{batch_loss / batch_counts:^11.6f} | \"\n",
        "                        f\"{batch_lab_loss / batch_counts:^15.6f} | \"\n",
        "                        f\"{batch_unl_loss / batch_counts :^13.6f} | \"\n",
        "                        f\"{'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\"\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    log(\n",
        "                        f\"{epoch_i + 1:^7} | {step:^12} | {batch_loss / batch_counts:^12.6f} | \"\n",
        "                        f\"{'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\"\n",
        "                    )\n",
        "\n",
        "                batch_loss, batch_lab_loss, batch_unl_loss, batch_counts = 0, 0, 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        if evaluate_during_training:\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            if is_student:\n",
        "                log(\"-\"*130)\n",
        "                log(\n",
        "                    f\"{epoch_i + 1:^7} | {'-':^14} | {'-':^16} | {avg_train_loss:^11.6f} | \"\n",
        "                    f\"{'-':^15} | {'-':^13}| {val_loss:^10.6f} | \"\n",
        "                    f\"{val_accuracy:^9.2f} | {time_elapsed:^9.2f}\"\n",
        "                )\n",
        "                log(\"-\"*130)\n",
        "            else: \n",
        "                log(\"-\"*80)\n",
        "                log(\n",
        "                    f\"{epoch_i + 1:^7} | {'-':^12} | {avg_train_loss:^12.6f} | \"\n",
        "                    f\"{val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\"\n",
        "                )\n",
        "                log(\"-\"*80)\n",
        "        log(\"\\n\")\n",
        "\n",
        "        historic_loss[\"loss\"].append(loss_list)\n",
        "        historic_loss[\"labeled_loss\"].append(lab_loss_list)\n",
        "        historic_loss[\"unlabeled_loss\"].append(unl_loss_list)\n",
        "        historic_loss[\"unl_steps\"].append(unl_step_list)\n",
        "        historic_loss[\"steps\"].append(step_list)\n",
        "\n",
        "    return historic_loss"
      ],
      "metadata": {
        "id": "rUx58DerkPMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        batch_inputs = {k: v.to(CFG.device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch_inputs)\n",
        "            logits = output.logits\n",
        "            loss = output.loss\n",
        "\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        labels = batch_inputs[\"labels\"]\n",
        "\n",
        "        accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "kaPgbHxMkeXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_predict(model, dataloader):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch_inputs = {k: v.to(CFG.device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch_inputs)\n",
        "            logits = output.logits\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "    labels = np.argmax(probs, axis=1)\n",
        "\n",
        "    return probs, labels"
      ],
      "metadata": {
        "id": "i193-JSWkhZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(model, test_dataloader):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    true_labels = []\n",
        "    history = {\"y_true\": [], \"y_pred\": [], \"logits_0\": [], \"logits_1\": []}\n",
        "    for batch in test_dataloader:\n",
        "        true_labels.extend(batch[\"labels\"].detach().cpu().numpy())\n",
        "        batch_inputs = {k: v.to(CFG.device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch_inputs)\n",
        "            logits = output.logits\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    clf_report = classification_report(true_labels, preds)\n",
        "    f1 = f1_score(true_labels, preds, average=\"macro\")\n",
        "\n",
        "    history[\"y_true\"] = true_labels\n",
        "    history[\"y_pred\"] = preds.tolist()\n",
        "    history[\"logits_0\"] = all_logits.detach().cpu().numpy()[:, 0]\n",
        "    history[\"logits_1\"] = all_logits.detach().cpu().numpy()[:, 1]\n",
        "\n",
        "    return clf_report, f1, history"
      ],
      "metadata": {
        "id": "HP8XeNaITUDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Noisy Student Helpers"
      ],
      "metadata": {
        "id": "d8Zay1ReHjQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noisy_loop(\n",
        "    train_dataloader,\n",
        "    dev_dataloader,\n",
        "    test_dataloader,\n",
        "    unlabeled_dataloader,\n",
        "    ):\n",
        "    attention_dropout = CFG.attention_dropout_proba\n",
        "    classifier_dropout = CFG.classifier_dropout_proba\n",
        "    confidence_threshold = CFG.min_pred_confidence\n",
        "\n",
        "    history = {}\n",
        "\n",
        "    teacher_model, teacher_optimizer, teacher_scheduler = initialize_model(\n",
        "      train_dataloader=train_dataloader,\n",
        "      attention_dropout=attention_dropout,\n",
        "      classifier_dropout=classifier_dropout\n",
        "    )\n",
        "\n",
        "    print(\"before training the teacher:\")\n",
        "    gpu_usage()\n",
        "    # train teacher (base classifier)\n",
        "    train_history = train(\n",
        "        model=teacher_model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        optimizer=teacher_optimizer,\n",
        "        scheduler=teacher_scheduler,\n",
        "        val_dataloader=dev_dataloader,\n",
        "        evaluate_during_training=True,\n",
        "        is_student=False\n",
        "    )\n",
        "    print(\"after training the teacher:\")\n",
        "    gpu_usage()\n",
        "\n",
        "    # eval teacher\n",
        "    log(\"Base Classifier Metrics:\")\n",
        "    clf_report, f1, eval_history = get_metrics(teacher_model, test_dataloader)\n",
        "    log(clf_report)\n",
        "    log(f\"F1 Score: {f1:.4}\")\n",
        "\n",
        "    history[\"base_model\"] = {\"train_history\": train_history, \"eval_history\": eval_history}\n",
        "\n",
        "    for i in range(CFG.noisy_student_iter):\n",
        "        # get new high confidence samples from augmented data\n",
        "        augmented_dataloader, num_new_examples_pos, num_new_examples_neg = get_high_confidence_augmented(\n",
        "            teacher_model,\n",
        "            unlabeled_dataloader,\n",
        "            min_confidence = confidence_threshold\n",
        "        )\n",
        "        print(\"after inference\")\n",
        "        gpu_usage()\n",
        "        train_steps = int(np.ceil(len(train_dataloader.dataset)/CFG.batch_size))\n",
        "        augmented_steps = int(np.ceil(len(augmented_dataloader.dataset)/CFG.batch_size))\n",
        "        unl_to_label_batch_ratio = int(np.ceil(augmented_steps/train_steps))\n",
        "        if unl_to_label_batch_ratio < 1:\n",
        "            raise Exception(\"Not enough new samples to train\")\n",
        "\n",
        "        # free teacher model from gpu\n",
        "        del teacher_model\n",
        "\n",
        "        # add model noise\n",
        "        attention_dropout += CFG.increase_attention_dropout\n",
        "        classifier_dropout += CFG.increase_classifier_dropout\n",
        "        confidence_threshold += CFG.increase_confidence_threshold\n",
        "\n",
        "        # define student\n",
        "        student_model, student_optimizer, student_scheduler = initialize_model(\n",
        "            train_dataloader=train_dataloader,\n",
        "            attention_dropout=attention_dropout,\n",
        "            classifier_dropout=classifier_dropout\n",
        "        )\n",
        "\n",
        "        print(\"before training the student\")\n",
        "        gpu_usage()\n",
        "        # train student\n",
        "        train_history = train(\n",
        "            model = student_model,\n",
        "            train_dataloader = train_dataloader,\n",
        "            optimizer = student_optimizer,\n",
        "            scheduler = student_scheduler,\n",
        "            val_dataloader = dev_dataloader,\n",
        "            evaluate_during_training = True,\n",
        "            is_student = True,\n",
        "            unl_to_label_batch_ratio=unl_to_label_batch_ratio,\n",
        "            unlabeled_dataloader=augmented_dataloader\n",
        "        )\n",
        "        print(\"after training the student\")\n",
        "        gpu_usage()\n",
        "        # eval student\n",
        "        log(\"Classifier Metrics:\")\n",
        "        clf_report, f1, eval_history = get_metrics(student_model, test_dataloader)\n",
        "        log(clf_report)\n",
        "        log(f\"F1 Score: {f1:.4}\")\n",
        "\n",
        "        teacher_model = student_model\n",
        "\n",
        "        history[f\"student_{i}\"] = {\n",
        "            \"train_history\": train_history,\n",
        "            \"eval_history\": eval_history\n",
        "        }\n",
        "        history[f\"student_{i}\"][\"train_history\"][\"num_new_examples_pos\"] = num_new_examples_pos\n",
        "        history[f\"student_{i}\"][\"train_history\"][\"num_new_examples_neg\"] = num_new_examples_neg\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "qciXg-oR0qCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_high_confidence_augmented(model, unlabeled_dataloader, min_confidence):\n",
        "    texts = []\n",
        "    augmented = []\n",
        "    labels = []\n",
        "    logits = []\n",
        "    for unl_batch in tqdm(unlabeled_dataloader):\n",
        "        # tokenize unlabeled batch\n",
        "        unl_inputs = tokenizer.batch_encode_plus(\n",
        "            unl_batch[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_seq_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # get model predictions\n",
        "        batch_inputs = {k: v.to(CFG.device) for k, v in unl_inputs.items()}\n",
        "        model.to(CFG.device)\n",
        "        with torch.no_grad():\n",
        "            unl_outputs = model(**batch_inputs)\n",
        "\n",
        "        unl_logits = unl_outputs.logits\n",
        "        batch_labels = unl_logits.argmax(dim=-1).cpu().detach().numpy()\n",
        "        \n",
        "        logits.append(unl_logits.cpu().detach().numpy())\n",
        "        texts.extend(unl_batch[\"text\"])\n",
        "        labels.extend(batch_labels)\n",
        "        augmented.extend(unl_batch[\"text_augmented\"])\n",
        "\n",
        "    logits = np.concatenate(logits)\n",
        "    # get all examples with high confidence\n",
        "    unl_softmax = softmax(logits, axis=1)\n",
        "    high_confidence_positive_idxs = np.where(unl_softmax[:,1] >= min_confidence)[0]\n",
        "    high_confidence_negative_idxs = np.where(unl_softmax[:,0] >= min_confidence)[0]\n",
        "\n",
        "    # select same amount of positives and negatives (limited by the class with least examples)\n",
        "    size = min(len(high_confidence_positive_idxs), len(high_confidence_negative_idxs))\n",
        "\n",
        "    if size > 0:\n",
        "        high_confidence_negative_idxs = np.random.choice(\n",
        "            high_confidence_negative_idxs,\n",
        "            size=size,\n",
        "            replace=False\n",
        "        )\n",
        "        high_confidence_positive_idxs = np.random.choice(\n",
        "            high_confidence_positive_idxs,\n",
        "            size=size,\n",
        "            replace=False\n",
        "        )\n",
        "        high_confidence_idxs = np.append(\n",
        "            high_confidence_positive_idxs,\n",
        "            high_confidence_negative_idxs\n",
        "        )\n",
        "        \n",
        "    # model predicted everything as one of the classes\n",
        "    # get 10.000 random samples from each class\n",
        "    else:\n",
        "        size = 10000\n",
        "        high_confidence_negative_idxs = np.random.choice(\n",
        "            np.where(unl_softmax[:,0] > 0.5)[0],\n",
        "            size=size,\n",
        "            replace=False\n",
        "        )\n",
        "        high_confidence_positive_idxs = np.random.choice(\n",
        "            np.where(unl_softmax[:,1] > 0.5)[0],\n",
        "            size=size,\n",
        "            replace=False\n",
        "        )\n",
        "        high_confidence_idxs = np.append(\n",
        "            high_confidence_positive_idxs,\n",
        "            high_confidence_negative_idxs\n",
        "        )\n",
        "\n",
        "    # get selected elements from each data field by their idxs\n",
        "    selected_text_augmented = list(\n",
        "        map(augmented.__getitem__, high_confidence_idxs.tolist())\n",
        "    )\n",
        "    selected_text = list(map(texts.__getitem__, high_confidence_idxs.tolist()))\n",
        "    selected_label = np.argmax(unl_softmax[high_confidence_idxs], axis=1)\n",
        "    selected_confidence = np.max(unl_softmax[high_confidence_idxs], axis=1)\n",
        "    \n",
        "    augmented_df = pd.DataFrame({\"text\": selected_text, \"text_augmented\": selected_text_augmented, \"label\": selected_label, \"confidence\": selected_confidence})\n",
        "    # augmented_df = pd.DataFrame({\"text\": texts, \"text_augmented\": augmented, \"label\": np.argmax(unl_softmax, axis=1), \"confidence\": np.max(unl_softmax, axis=1)})\n",
        "    # augmented_df.to_csv(\"predicted-convabuse.csv\", index=False)\n",
        "\n",
        "    augmentedset = AugmentedDataset(augmented_df, labels=labels)\n",
        "\n",
        "    augmented_sampler = RandomSampler(augmentedset)\n",
        "    augmented_dataloader = DataLoader(\n",
        "        augmentedset,\n",
        "        sampler=augmented_sampler,\n",
        "        batch_size=CFG.batch_size\n",
        "    )\n",
        "    amnt_new_samples_pos = len(augmented_df[augmented_df[\"label\"] == 1])\n",
        "    amnt_new_samples_neg = len(augmented_df[augmented_df[\"label\"] == 0])\n",
        "    log(\n",
        "        \"Added to train set:\\n\"\n",
        "        f\"\\tNew + samples: {amnt_new_samples_pos}\\n\"\n",
        "        f\"\\tNew - Samples: {amnt_new_samples_neg}\"\n",
        "    )\n",
        "\n",
        "    return augmented_dataloader, amnt_new_samples_pos, amnt_new_samples_neg\n"
      ],
      "metadata": {
        "id": "65_3S6P7MWU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### General "
      ],
      "metadata": {
        "id": "CeE102MTgtno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log(text):\n",
        "    print(text)\n",
        "    logfile.write(text + \"\\n\")"
      ],
      "metadata": {
        "id": "QCHB07Pggwem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)"
      ],
      "metadata": {
        "id": "OS978bXekVPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Experiment "
      ],
      "metadata": {
        "id": "CJmNw9Argf1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Setup"
      ],
      "metadata": {
        "id": "VWVV42HdFcDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuration"
      ],
      "metadata": {
        "id": "nGzryhiJInae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # experiment parameters\n",
        "    dataset_name = \"measuring_hate_speech\"\n",
        "    experiment_id = \"01\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "    seed = 42\n",
        "    data_path = \"drive/MyDrive/NoisyToxic/data/\"\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    num_labels = 2\n",
        "    few_shot = True\n",
        "    num_split = 0\n",
        "    # bert parameters\n",
        "    pretrained_bert_name = \"bert-base-cased\"\n",
        "    num_train_epochs = 2\n",
        "    batch_size = 32\n",
        "    max_seq_len = 128\n",
        "    learning_rate = 5e-5\n",
        "    warmup_ratio = 0.15\n",
        "    weight_decay = 1e-2\n",
        "    classifier_dropout_proba = 0.1\n",
        "    attention_dropout_proba = 0.1\n",
        "    clip_grad = True\n",
        "    # noisystudent parameters\n",
        "    increase_classifier_dropout = 0.15\n",
        "    increase_attention_dropout = 0.15\n",
        "    increase_confidence_threshold = 0.0\n",
        "    noisy_student_iter = 3\n",
        "    min_pred_confidence = 0.9\n",
        "    augmented_data = True\n",
        "    def to_json():\n",
        "        return {\n",
        "            \"dataset_name\": CFG.dataset_name,\n",
        "            \"experiment_id\": CFG.experiment_id,\n",
        "            \"timestamp\": CFG.timestamp,\n",
        "            \"seed\": CFG.seed,\n",
        "            \"data_path\": CFG.data_path,\n",
        "            \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
        "            \"num_labels\" : CFG.num_labels,\n",
        "            \"few_shot\": CFG.few_shot,\n",
        "            \"num_splt\": CFG.num_split,\n",
        "            \"pretrained_bert_name\": CFG.pretrained_bert_name,\n",
        "            \"num_train_epochs\": CFG.num_train_epochs,\n",
        "            \"batch_size\": CFG.batch_size,\n",
        "            \"max_seq_len\": CFG.max_seq_len,\n",
        "            \"learning_rate\": CFG.learning_rate,\n",
        "            \"warmup_ratio\": CFG.warmup_ratio,\n",
        "            \"weight_decay\": CFG.weight_decay,\n",
        "            \"classifier_dropout_proba\": CFG.classifier_dropout_proba,\n",
        "            \"attention_dropout_proba\": CFG.attention_dropout_proba,\n",
        "            \"clip_grad\": CFG.clip_grad,\n",
        "            \"increase_classifier_dropout\": CFG.increase_classifier_dropout,\n",
        "            \"increase_attention_dropout\": CFG.increase_attention_dropout,\n",
        "            \"increase_confidence_threshold\": CFG.increase_confidence_threshold,\n",
        "            \"noisy_student_iter\": CFG.noisy_student_iter,\n",
        "            \"min_pred_confidence\": CFG.min_pred_confidence,\n",
        "            \"augmented_data\" : CFG.augmented_data,\n",
        "        }"
      ],
      "metadata": {
        "id": "nEbWRFu66LmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = f\"experiment.log\"\n",
        "exp_path = f\"{os.path.join(CFG.data_path, 'logs', CFG.dataset_name)}\"\n",
        "if CFG.few_shot:\n",
        "    exp_path = f\"{os.path.join(exp_path, 'few_shot', CFG.experiment_id)}\"\n",
        "else:\n",
        "    exp_path = f\"{os.path.join(exp_path, CFG.experiment_id)}\"\n",
        "\n",
        "if not os.path.exists(exp_path):\n",
        "    os.mkdir(exp_path)\n",
        "logfile = open(f\"{os.path.join(exp_path, fname)}\", \"w\")\n",
        "log(json.dumps(CFG.to_json(), indent=4)[1:-1])"
      ],
      "metadata": {
        "id": "qzhmWqv0-zJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(CFG.seed)"
      ],
      "metadata": {
        "id": "MyuSg9wrqjdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data"
      ],
      "metadata": {
        "id": "yXK4Ex4fhoxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loading Datasets"
      ],
      "metadata": {
        "id": "b0oihhznh7_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, dev_df, test_df, unlabeled_df = load_dataset(CFG.few_shot, 0)"
      ],
      "metadata": {
        "id": "6ax3jcImLNuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenizing"
      ],
      "metadata": {
        "id": "xHUbuzp9ElsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CFG.pretrained_bert_name == \"vinai/bertweet-base\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        CFG.pretrained_bert_name,\n",
        "        normalize=True\n",
        "    )\n",
        "else:\n",
        "     tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_bert_name)"
      ],
      "metadata": {
        "id": "_H0XRkD29qSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = tokenizer(\n",
        "    train_df.iloc[:, 0].astype(\"str\").to_list(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=CFG.max_seq_len,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "trainset = BertDataset(tokenized_train, labels=train_df.iloc[:, 1].to_list())"
      ],
      "metadata": {
        "id": "H_3OJnj394za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dev_df is not None:\n",
        "    tokenized_dev = tokenizer(\n",
        "        dev_df.iloc[:, 0].astype(\"str\").to_list(),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=CFG.max_seq_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    devset = BertDataset(tokenized_dev, labels=dev_df.iloc[:, 1].to_list())"
      ],
      "metadata": {
        "id": "V1THllGnN1_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test = tokenizer(\n",
        "    test_df.iloc[:, 0].astype(\"str\").to_list(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=CFG.max_seq_len,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "testset = BertDataset(tokenized_test, labels=test_df.iloc[:, 1].to_list())"
      ],
      "metadata": {
        "id": "iKt_mbGqn4iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeledset = AugmentedDataset(unlabeled_df)"
      ],
      "metadata": {
        "id": "gbWvCcAyoEYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataloaders"
      ],
      "metadata": {
        "id": "6aSDHdDzEo-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler = RandomSampler(trainset)\n",
        "train_dataloader = DataLoader(trainset, sampler=train_sampler, batch_size=CFG.batch_size)\n",
        "\n",
        "test_sampler = SequentialSampler(testset)\n",
        "test_dataloader = DataLoader(testset, sampler=test_sampler, batch_size=CFG.batch_size)\n",
        "\n",
        "if dev_df is not None:\n",
        "    dev_sampler = SequentialSampler(devset)\n",
        "    dev_dataloader = DataLoader(devset, sampler=dev_sampler, batch_size=CFG.batch_size)\n",
        "else:\n",
        "    dev_dataloader = test_dataloader \n",
        "\n",
        "unlabeled_sampler = RandomSampler(unlabeledset)\n",
        "unlabeled_dataloader = DataLoader(unlabeledset, sampler=unlabeled_sampler, batch_size=CFG.batch_size)"
      ],
      "metadata": {
        "id": "tQmVpg75797d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Classification"
      ],
      "metadata": {
        "id": "XdUdYEiWiDhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df = pd.DataFrame(columns=[\"text\", \"text_augmented\"])"
      ],
      "metadata": {
        "id": "epnFdIJW367P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = noisy_loop(\n",
        "    train_dataloader,\n",
        "    dev_dataloader,\n",
        "    test_dataloader,\n",
        "    unlabeled_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "z1d8idd-EzmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logfile.close()"
      ],
      "metadata": {
        "id": "GiuOkRrZ-kgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history[\"configs\"] = CFG.to_json()\n",
        "fname = \"experiment.pkl\"\n",
        "with open(f\"{os.path.join(CFG.data_path, 'logs', CFG.dataset_name, CFG.experiment_id, fname)}\", \"ab\") as jsonfile:\n",
        "    pickle.dump(history, jsonfile)"
      ],
      "metadata": {
        "id": "uAIxy9JbjQ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for dataset in [\"olidv1\", \"measuring_hate_speech\", \"davidson\", \"convabuse\"]:\n",
        "    print(dataset)\n",
        "    CFG.dataset_name = dataset\n",
        "    train_df, dev_df, test_df, unlabeled_df = load_dataset()\n",
        "\n",
        "    if CFG.pretrained_bert_name == \"vinai/bertweet-base\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "        CFG.pretrained_bert_name,\n",
        "        normalize=True\n",
        "    )\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_bert_name)\n",
        "\n",
        "    teacher_model, teacher_optimizer, teacher_scheduler = initialize_model(\n",
        "      train_dataloader=train_dataloader,\n",
        "      attention_dropout=0.1,\n",
        "      classifier_dropout=0.1\n",
        "    )\n",
        "\n",
        "    tokenized_train = tokenizer(\n",
        "    train_df.iloc[:, 0].astype(\"str\").to_list(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=CFG.max_seq_len,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    trainset = BertDataset(tokenized_train, labels=train_df.iloc[:, 1].to_list())\n",
        "\n",
        "    if dev_df is not None:\n",
        "        tokenized_dev = tokenizer(\n",
        "        dev_df.iloc[:, 0].astype(\"str\").to_list(),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=CFG.max_seq_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "        devset = BertDataset(tokenized_dev, labels=dev_df.iloc[:, 1].to_list())\n",
        "\n",
        "        \n",
        "    tokenized_test = tokenizer(\n",
        "    test_df.iloc[:, 0].astype(\"str\").to_list(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=CFG.max_seq_len,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "    testset = BertDataset(tokenized_test, labels=test_df.iloc[:, 1].to_list())\n",
        "    unlabeledset = AugmentedDataset(unlabeled_df)\n",
        "\n",
        "    train_sampler = RandomSampler(trainset)\n",
        "    train_dataloader = DataLoader(trainset, sampler=train_sampler, batch_size=CFG.batch_size)\n",
        "\n",
        "    test_sampler = SequentialSampler(testset)\n",
        "    test_dataloader = DataLoader(testset, sampler=test_sampler, batch_size=CFG.batch_size)\n",
        "\n",
        "    if dev_df is not None:\n",
        "        dev_sampler = SequentialSampler(devset)\n",
        "        dev_dataloader = DataLoader(devset, sampler=dev_sampler, batch_size=CFG.batch_size)\n",
        "    else:\n",
        "        dev_dataloader = test_dataloader \n",
        "\n",
        "    # unlabeled_sampler = RandomSampler(unlabeledset)\n",
        "    unlabeled_sampler = SequentialSampler(unlabeledset)\n",
        "    unlabeled_dataloader = DataLoader(unlabeledset, sampler=unlabeled_sampler, batch_size=CFG.batch_size)\n",
        "\n",
        "    train_history = train(\n",
        "        model=teacher_model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        optimizer=teacher_optimizer,\n",
        "        scheduler=teacher_scheduler,\n",
        "        val_dataloader=dev_dataloader,\n",
        "        evaluate_during_training=True,\n",
        "        is_student=False\n",
        "    )\n",
        "\n",
        "    # eval teacher\n",
        "    log(\"Base Classifier Metrics:\")\n",
        "    clf_report, f1, eval_history = get_metrics(teacher_model, test_dataloader)\n",
        "    log(clf_report)\n",
        "    log(f\"F1 Score: {f1:.4}\")\n",
        "\n",
        "    # get new high confidence samples from augmented data\n",
        "    augmented_df = get_high_confidence_augmented(\n",
        "        teacher_model,\n",
        "        unlabeled_dataloader,\n",
        "        min_confidence = 0.8\n",
        "    )\n",
        "\n",
        "    labeled_df[\"text\"] = augmented_df[\"text\"]\n",
        "    labeled_df[\"text_augmented\"] = augmented_df[\"text_augmented\"]\n",
        "    labeled_df[f\"{dataset}_label\"] = augmented_df[\"label\"]\n",
        "    labeled_df[f\"{dataset}_confidence\"] = augmented_df[\"confidence\"]\n"
      ],
      "metadata": {
        "id": "9XlGN2Ht3ddI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_RlatoipQKyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}