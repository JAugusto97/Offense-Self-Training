{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAugusto97/noisystudentNLP/blob/main/Torch_NoisyToxic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihYgOFMzMHZk"
      },
      "outputs": [],
      "source": [
        "#https://skimai.com/fine-tuning-bert-for-sentiment-analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgCCQCUAuz-Z"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fdxqOyBcvXZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers emoji --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGwYu4ZCWxfA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba3IPaucco-e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AdamW, get_scheduler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMro1WI2cF4w"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEbWRFu66LmS"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    # pretrained_bert_name = \"vinai/bertweet-base\"\n",
        "    pretrained_bert_name = \"distilbert-base-cased\"\n",
        "    seed = 7\n",
        "    datasets_path = \"drive/MyDrive/NoisyToxic/data/\"\n",
        "    steps = 3\n",
        "    min_confidence = 0.8    \n",
        "    hidden_dim = 128\n",
        "    train_epochs = 1\n",
        "    do_lower_case = False\n",
        "    batch_size = 8\n",
        "    max_seq_len = 32\n",
        "    dropout_proba = 0.1\n",
        "    increase_dropout_step = 0.1\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "    def to_json():\n",
        "        return {\n",
        "            \"pretrained_bert_name\": CFG.pretrained_bert_name,\n",
        "            \"seed\": CFG.seed,\n",
        "            \"datasets_path\": CFG.datasets_path,\n",
        "            \"steps\": CFG.steps,\n",
        "            \"hidden_dim\": CFG.hidden_dim,\n",
        "            \"min_confidence\": CFG.min_confidence,\n",
        "            \"train_epochs\": CFG.train_epochs,\n",
        "            \"do_lower_case\": CFG.do_lower_case,\n",
        "            \"batch_size\": CFG.batch_size,\n",
        "            \"max_seq_len\": CFG.max_seq_len,\n",
        "            \"dropout_proba\": CFG.dropout_proba,\n",
        "            \"increase_dropout_step\": CFG.increase_dropout_step,\n",
        "            \"timestamp\": CFG.timestamp\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzhmWqv0-zJ7"
      },
      "outputs": [],
      "source": [
        "fname = f\"EXP_{CFG.timestamp}.log\"\n",
        "log = open(f\"{os.path.join( fname)}\", \"w\")\n",
        "plog(json.dumps(CFG.to_json(), indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ax3jcImLNuD"
      },
      "outputs": [],
      "source": [
        "train_df, test_df, unlabeled_df = load_olid(CFG.datasets_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H0XRkD29qSI"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CFG.pretrained_bert_name,\n",
        "    # use_fast=True,\n",
        "    # normalize=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_3OJnj394za"
      },
      "outputs": [],
      "source": [
        "tokenized_train = batch_tokenize(train_df[\"text\"].to_list())\n",
        "trainset = OlidDataset(tokenized_train, labels=train_df[\"toxic\"].to_list())\n",
        "\n",
        "tokenized_test = batch_tokenize(test_df[\"text\"].to_list())\n",
        "testset = OlidDataset(tokenized_test, labels=test_df[\"toxic\"].to_list())\n",
        "\n",
        "unlabeledset = UnlabeledDataset(unlabeled_df, labels=[0 for i in range(len(unlabeled_df))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQmVpg75797d"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(trainset, shuffle=True, batch_size=CFG.batch_size)\n",
        "test_dataloader = DataLoader(testset, batch_size=CFG.batch_size)\n",
        "\n",
        "unl_batch_size = (len(unlabeled_df)//(len(train_df)//CFG.batch_size))\n",
        "unlabeled_dataloader = DataLoader(unlabeledset, shuffle=True, batch_size=unl_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKApH7xRWSAh"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "set_seed(CFG.seed)\n",
        "\n",
        "student_model, student_optimizer, student_scheduler = initialize_model(\n",
        "    pretrained_bert_name = CFG.pretrained_bert_name,\n",
        "    hidden_dim = CFG.hidden_dim,\n",
        "    n_labels = 2,\n",
        "    dropout_proba = CFG.dropout_proba+0.1, \n",
        "    epochs=CFG.train_epochs\n",
        ")\n",
        "\n",
        "teacher_model, teacher_optimizer, teacher_scheduler = initialize_model(\n",
        "    pretrained_bert_name = CFG.pretrained_bert_name,\n",
        "    hidden_dim = CFG.hidden_dim,\n",
        "    n_labels = 2,\n",
        "    dropout_proba = CFG.dropout_proba, \n",
        "    epochs=CFG.train_epochs\n",
        ")\n",
        "\n",
        "# for i in range(CFG.steps):\n",
        "teacher_model.to(device)\n",
        "teacher_model.train()\n",
        "train(\n",
        "    model = teacher_model,\n",
        "    train_dataloader = train_dataloader,\n",
        "    epochs = CFG.train_epochs,\n",
        "    optimizer = teacher_optimizer,\n",
        "    scheduler = teacher_scheduler,\n",
        "    val_dataloader = test_dataloader,\n",
        "    evaluate_during_training=True,\n",
        "    is_student=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0htn4PNYHCm6"
      },
      "outputs": [],
      "source": [
        "teacher_model.eval()\n",
        "probas, preds = bert_predict(teacher_model, test_dataloader)\n",
        "print(classification_report(test_df[\"toxic\"], preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfXqXZa2U1sm"
      },
      "outputs": [],
      "source": [
        "texts = []\n",
        "labels = []\n",
        "augmented = []\n",
        "for unl_batch in unlabeled_dataloader:\n",
        "    unl_inputs = batch_tokenize(unl_batch[\"text\"])\n",
        "\n",
        "    unl_input_ids = torch.LongTensor(unl_inputs['input_ids']).to(device)\n",
        "    unl_attention_mask = torch.LongTensor(unl_inputs['attention_mask']).to(device)\n",
        "    unl_logits = teacher_model(unl_input_ids, unl_attention_mask)\n",
        "    unl_softmax = F.softmax(unl_logits).cpu().detach().numpy()\n",
        "\n",
        "    high_confidence_positive_idxs = np.where(unl_softmax[:,1] >= CFG.min_confidence)[0] # high confidence positive preds\n",
        "    high_confidence_negative_idxs = np.where(unl_softmax[:,0] >= CFG.min_confidence)[0]\n",
        "    high_confidence_negative_idxs = np.random.choice(high_confidence_negative_idxs, size=len(high_confidence_positive_idxs), replace=False)\n",
        "\n",
        "    high_confidence_idxs = np.append(high_confidence_positive_idxs, high_confidence_negative_idxs)\n",
        "\n",
        "    high_confidence_augmented = list(map(unl_batch[\"text_augmented\"].__getitem__, high_confidence_idxs.tolist()))\n",
        "    high_confidence_text = list(map(unl_batch[\"text\"].__getitem__, high_confidence_idxs.tolist()))\n",
        "    unl_labels = np.argmax(unl_softmax[high_confidence_idxs], axis=1)\n",
        "\n",
        "    texts.extend(high_confidence_text)\n",
        "    labels.extend(unl_labels)\n",
        "    augmented.extend(high_confidence_augmented)\n",
        "\n",
        "df = pd.DataFrame({\"text\": texts, \"text_augmented\": augmented})\n",
        "processed_dataset = UnlabeledDataset(df, labels=labels)\n",
        "\n",
        "unl_batch_size = len(df)//(len(train_df)//CFG.batch_size)\n",
        "if unl_batch_size <= 0:\n",
        "    unl_batch_size = 1\n",
        "\n",
        "augmented_dataloader = DataLoader(processed_dataset, shuffle=True, batch_size=unl_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gauhk9_r_2mf"
      },
      "outputs": [],
      "source": [
        "del teacher_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G74Oo10Vf4Ni"
      },
      "outputs": [],
      "source": [
        "student_model.to(device)\n",
        "student_model.train()\n",
        "train(\n",
        "    model = student_model,\n",
        "    train_dataloader = train_dataloader,\n",
        "    epochs = CFG.train_epochs,\n",
        "    optimizer = student_optimizer,\n",
        "    scheduler = student_scheduler,\n",
        "    val_dataloader = test_dataloader,\n",
        "    evaluate_during_training=True,\n",
        "    is_student=True,\n",
        "    unlabeled_dataloader=augmented_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4rflZG7S_cU"
      },
      "outputs": [],
      "source": [
        "student_model.eval()\n",
        "probas, pred = bert_predict(student_model, test_dataloader)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Ik6cFp-eMP"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_df[\"toxic\"], pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGaZ7njv-nPi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Torch_NoisyToxic.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
